<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Session 3.2: Topic modeling</title>

<script src="site_libs/header-attrs-2.15/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/clipboard-1.7.1/clipboard.min.js"></script>
<link href="site_libs/primer-tooltips-1.4.0/build.css" rel="stylesheet" />
<link href="site_libs/klippy-0.0.0.9500/css/klippy.min.css" rel="stylesheet" />
<script src="site_libs/klippy-0.0.0.9500/js/klippy.min.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>





<style type="text/css">
/* for pandoc --citeproc since 2.11 */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="styles.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-inverse  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html"></a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">
    <span class="fa fa-home"></span>
     
    Overview
  </a>
</li>
<li>
  <a href="pensum.html">
    <span class="fa fa-list-ul"></span>
     
    Reading List
  </a>
</li>
<li>
  <a href="schedule.html">
    <span class="fa fa-info"></span>
     
    Schedule
  </a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    <span class="fa fa-book"></span>
     
    Quantitative methods
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Day 1</li>
    <li>
      <a href="1_1-DescStat.html">Descriptive statistics</a>
    </li>
    <li class="dropdown-header">Day 2</li>
    <li>
      <a href="2_1-CorrAnalysis.html">Correlation analysis</a>
    </li>
    <li class="dropdown-header">Day 3</li>
    <li>
      <a href="3_1-RegAnalysis.html">Regression analysis</a>
    </li>
    <li class="dropdown-header">Day 4</li>
    <li>
      <a href="4_1-Uncertainty.html">Uncertainty</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    <span class="fa fa-book"></span>
     
    Computational text analysis
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Day 1</li>
    <li>
      <a href="1_2-readingcleaning.html">Reading and cleaning text data</a>
    </li>
    <li class="dropdown-header">Day 2</li>
    <li>
      <a href="2_2-WordFreq.html">Word frequency and dictionary methods</a>
    </li>
    <li class="dropdown-header">Day 3</li>
    <li>
      <a href="3_2-Topicmodelling.html">Topic modelling</a>
    </li>
    <li class="dropdown-header">Day 4</li>
    <li>
      <a href="4_2-WordEmbedding.html">Word embedding</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Session 3.2: Topic modeling</h1>

</div>


<script>
  addClassKlippyTo("pre.r, pre.markdown");
  addKlippy('right', 'top', 'auto', '1', 'Copy code', 'Copied!');
</script>
<pre class="r"><code>library(tidyverse)
library(tidytext)
library(stm)
library(topicmodels)
library(quanteda)
library(quanteda.textstats)
library(quanteda.textplots)</code></pre>
<p>Today’s script is <a
href="data/script_wednesday_text.R">here</a>.</p>
<div id="reading-into-topic-modeling-packages-topicmodels"
class="section level1" number="1">
<h1><span class="header-section-number">1</span> Reading into topic
modeling packages (topicmodels)</h1>
<p>There are a number of topic model instantiations in R. We will go
through one of the more popular ones – <code>topicmodels</code> which
plays very nicely with the tidyverse.<a href="#fn1" class="footnote-ref"
id="fnref1"><sup>1</sup></a></p>
<p><code>topicmodels</code>’s main topic model function is
<code>LDA()</code>, which stands for Latent Dirichlet Allocation, a type
of topic model and often used as shorthand for topic models in general.
It takes a DTM as input and gives us an object of class <code>LDA</code>
as output, which we can then analyze and visualize in the tidyverse.
There are many points where we can customize, adjust parameters and so
on but the one we must specify is the number of topics. This is
something that often takes some fiddling with. Unless you have reason to
think that the number of topics is extremely limited in a certain corpus
one generally uses between ~20-50 topics. The other parameter it makes
sense to think of prior to, or under, analysis is document size. As
we’ve seen, a DTM will break up a text without concern for order within
individual documents. So large documents will be extremely generalized
in a DTM. It could well be reasonable to break up books, for example, by
chapter. We could go more finer grained as well – chunking by paragraph
might make sense sometimes, too. Much will depend on the corpus and
object of analysis. Experiment and see what leads to the most
understandable and coherent topics.</p>
<pre class="r"><code>options(stringsAsFactors = FALSE)
library(tidyverse)
library(tidytext)
library(topicmodels)
# read in the dataframe into R as normal
nobel_tidy &lt;- read_rds(&quot;data/nobel_stemmed.Rds&quot;) %&gt;%
  select(Year, Laureate, word_stem) %&gt;%
  rename(Year = Year, Laureate = Laureate, words = word_stem)
# transform dataframe to DTM
nobel_dtm &lt;- nobel_tidy %&gt;%
  group_by(Year) %&gt;%
  count(words, sort = TRUE) %&gt;%
  cast_dtm(Year, words, n)</code></pre>
<p>There are many points where we can customize, adjust parameters and
so on but the one we must specify is the number of topics. This is
something that often takes some fiddling with. Unless you have reason to
think that the number of topics is extremely limited in a certain corpus
one generally uses between ~15-50 topics (very roughly).</p>
<p>Another parameter it makes sense to think of prior to, or under,
analysis is document size. As we’ve seen, a DTM will break up a text
without concern for order within individual documents. So large
documents will be extremely generalized in a DTM. It could well be
reasonable to break up books, for example, by chapter. We could go more
finer grained as well – chunking by paragraph might make sense
sometimes, too. Much will depend on the corpus and object of analysis.
Experiment and see what leads to the most understandable and coherent
topics.</p>
<p>We are also using the corpus that we have already cleaned and removed
stopwords from. We might also question if certain words are turning up
so much in every document that they won’t add anything to the topics
that the topic model finds (removing frequently appearing words will
also reduce the time it takes for the algorithm to fit the topic model).
We might consider if, in the Nobel corpus, the word “nobel” will add
anything to any of the topics, especially if we are treating the
documents as the speeches as a whole. It might or might or not, topic
models take some experimentation.</p>
<p>Lastly, the alpha parameter controls how much documents come to be
dominated by one or few topics or if the topics are more evenly
distributed over documents. This parameter is automatically optimized by
the algorithm if the user does not set it, but often algorithmic
optimization does not lead to the best model fit from the standpoint of
a human. This model tends toward a low alpha and very uneven topic
spread so we’ll set it ourselves. Again, this is something the analyst
must experiment with.</p>
<pre class="r"><code>k = 15
alpha = 2
nobel_tm &lt;- LDA(nobel_dtm, k = k, alpha = alpha)</code></pre>
<p>Fitting the model involves us telling R finding a distributions that
best match the corpus we have given the general structural assumptions
the topic model takes. There are different methods for doing this and
they might take a while. We are interested in two distributions: theta
(<span class="math inline">\(\theta\)</span>) – the proportion of each
document devoted to which topics, and beta (<span
class="math inline">\(\beta\)</span>) – the proportion of each topic
made up by which words (see the presentation <a
href="Presentations/Topic_models.pdf">pdf</a> for details).</p>
<p>Let’s first take a look at the output of the topic model. We call
<code>posterior()</code> to get these so-called posterior
distributions.</p>
<pre class="r"><code>str(posterior(nobel_tm))</code></pre>
<pre><code>## List of 2
##  $ terms : num [1:15, 1:8435] 4.69e-04 4.68e-306 1.04e-04 1.64e-02 7.46e-04 ...
##   ..- attr(*, &quot;dimnames&quot;)=List of 2
##   .. ..$ : chr [1:15] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ...
##   .. ..$ : chr [1:8435] &quot;refuge&quot; &quot;nuclear&quot; &quot;weapon&quot; &quot;war&quot; ...
##  $ topics: num [1:94, 1:15] 7.02e-06 9.99e-06 1.01e-05 7.64e-06 7.64e-06 ...
##   ..- attr(*, &quot;dimnames&quot;)=List of 2
##   .. ..$ : chr [1:94] &quot;1981&quot; &quot;2017&quot; &quot;1954&quot; &quot;1925&quot; ...
##   .. ..$ : chr [1:15] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ...</code></pre>
<p>If you call <code>str()</code> on this object you see
<code>topicmodels</code> has returned two distributions, one called
<code>term</code> that is made up of a matrix of the twenty topics on
one axis and the 8063 unique words in the corpus on the other, with each
entry indicating likelihood of that word turning up given the topic (we
might think of this as the proportion of the topic taken up by each word
in the corpus). It is a probability distribution so each words
probability within a given topic has to sum to 1. This is the
<code>beta</code> matrix. The topics distribution we see is a matrix
size 92 x 20, the likelihood of each document (speech) containing each
of 20 topics – also summing to 1 within each document and that we might
think of as proportions. So what do we do with this?</p>
<p>The most useful thing to look at straight away are the highest words
in each topic – do the topics make sense to a human?</p>
<pre class="r"><code>terms(nobel_tm, 15)</code></pre>
<pre><code>##       Topic 1   Topic 2     Topic 3   Topic 4    Topic 5     Topic 6    
##  [1,] &quot;peac&quot;    &quot;peac&quot;      &quot;peac&quot;    &quot;peac&quot;     &quot;peac&quot;      &quot;peac&quot;     
##  [2,] &quot;countri&quot; &quot;nuclear&quot;   &quot;nation&quot;  &quot;war&quot;      &quot;countri&quot;   &quot;world&quot;    
##  [3,] &quot;right&quot;   &quot;weapon&quot;    &quot;war&quot;     &quot;refuge&quot;   &quot;war&quot;       &quot;prize&quot;    
##  [4,] &quot;nation&quot;  &quot;world&quot;     &quot;world&quot;   &quot;intern&quot;   &quot;world&quot;     &quot;women&quot;    
##  [5,] &quot;human&quot;   &quot;war&quot;       &quot;countri&quot; &quot;organ&quot;    &quot;ilo&quot;       &quot;nobel&quot;    
##  [6,] &quot;declar&quot;  &quot;intern&quot;    &quot;unit&quot;    &quot;countri&quot;  &quot;prize&quot;     &quot;countri&quot;  
##  [7,] &quot;peopl&quot;   &quot;human&quot;     &quot;intern&quot;  &quot;quaker&quot;   &quot;nobel&quot;     &quot;human&quot;    
##  [8,] &quot;south&quot;   &quot;peopl&quot;     &quot;time&quot;    &quot;women&quot;    &quot;intern&quot;    &quot;peopl&quot;    
##  [9,] &quot;unicef&quot;  &quot;prize&quot;     &quot;confer&quot;  &quot;nation&quot;   &quot;nation&quot;    &quot;develop&quot;  
## [10,] &quot;world&quot;   &quot;nation&quot;    &quot;polici&quot;  &quot;peopl&quot;    &quot;peopl&quot;     &quot;right&quot;    
## [11,] &quot;develop&quot; &quot;agreement&quot; &quot;presid&quot;  &quot;offic&quot;    &quot;govern&quot;    &quot;committe&quot; 
## [12,] &quot;african&quot; &quot;paul&quot;      &quot;pearson&quot; &quot;life&quot;     &quot;organ&quot;     &quot;wheat&quot;    
## [13,] &quot;freedom&quot; &quot;power&quot;     &quot;prize&quot;   &quot;movement&quot; &quot;human&quot;     &quot;nation&quot;   
## [14,] &quot;organ&quot;   &quot;time&quot;      &quot;lester&quot;  &quot;baker&quot;    &quot;agreement&quot; &quot;polit&quot;    
## [15,] &quot;white&quot;   &quot;test&quot;      &quot;econom&quot;  &quot;unit&quot;     &quot;develop&quot;   &quot;norwegian&quot;
##       Topic 7   Topic 8   Topic 9     Topic 10   Topic 11   Topic 12      
##  [1,] &quot;war&quot;     &quot;refuge&quot;  &quot;peac&quot;      &quot;war&quot;      &quot;peac&quot;     &quot;nation&quot;      
##  [2,] &quot;nation&quot;  &quot;war&quot;     &quot;nobel&quot;     &quot;peac&quot;     &quot;human&quot;    &quot;peac&quot;        
##  [3,] &quot;peac&quot;    &quot;countri&quot; &quot;prize&quot;     &quot;nation&quot;   &quot;prize&quot;    &quot;american&quot;    
##  [4,] &quot;time&quot;    &quot;nation&quot;  &quot;peopl&quot;     &quot;unit&quot;     &quot;peopl&quot;    &quot;unit&quot;        
##  [5,] &quot;life&quot;    &quot;offic&quot;   &quot;norwegian&quot; &quot;world&quot;    &quot;nobel&quot;    &quot;congo&quot;       
##  [6,] &quot;world&quot;   &quot;marshal&quot; &quot;nation&quot;    &quot;intern&quot;   &quot;world&quot;    &quot;war&quot;         
##  [7,] &quot;leagu&quot;   &quot;peac&quot;    &quot;award&quot;     &quot;organ&quot;    &quot;polit&quot;    &quot;kim&quot;         
##  [8,] &quot;europ&quot;   &quot;world&quot;   &quot;parti&quot;     &quot;committe&quot; &quot;award&quot;    &quot;intern&quot;      
##  [9,] &quot;germani&quot; &quot;nansen&quot;  &quot;process&quot;   &quot;red&quot;      &quot;san&quot;      &quot;hammarskjöld&quot;
## [10,] &quot;peopl&quot;   &quot;peopl&quot;   &quot;conflict&quot;  &quot;cross&quot;    &quot;committe&quot; &quot;secretari&quot;   
## [11,] &quot;polici&quot;  &quot;prize&quot;   &quot;countri&quot;   &quot;countri&quot;  &quot;mother&quot;   &quot;aid&quot;         
## [12,] &quot;bunch&quot;   &quot;time&quot;    &quot;committe&quot;  &quot;leagu&quot;    &quot;right&quot;    &quot;pact&quot;        
## [13,] &quot;countri&quot; &quot;govern&quot;  &quot;ahtisaari&quot; &quot;prize&quot;    &quot;life&quot;     &quot;polit&quot;       
## [14,] &quot;franc&quot;   &quot;award&quot;   &quot;war&quot;       &quot;american&quot; &quot;respect&quot;  &quot;countri&quot;     
## [15,] &quot;power&quot;   &quot;polit&quot;   &quot;polit&quot;     &quot;confer&quot;   &quot;person&quot;   &quot;lama&quot;        
##       Topic 13  Topic 14    Topic 15   
##  [1,] &quot;leagu&quot;   &quot;peac&quot;      &quot;nuclear&quot;  
##  [2,] &quot;nation&quot;  &quot;prize&quot;     &quot;peac&quot;     
##  [3,] &quot;war&quot;     &quot;nobel&quot;     &quot;weapon&quot;   
##  [4,] &quot;peac&quot;    &quot;world&quot;     &quot;prize&quot;    
##  [5,] &quot;disarma&quot; &quot;right&quot;     &quot;nobel&quot;    
##  [6,] &quot;intern&quot;  &quot;human&quot;     &quot;intern&quot;   
##  [7,] &quot;time&quot;    &quot;peopl&quot;     &quot;award&quot;    
##  [8,] &quot;world&quot;   &quot;award&quot;     &quot;world&quot;    
##  [9,] &quot;public&quot;  &quot;countri&quot;   &quot;war&quot;      
## [10,] &quot;govern&quot;  &quot;committe&quot;  &quot;committe&quot; 
## [11,] &quot;cecil&quot;   &quot;war&quot;       &quot;presid&quot;   
## [12,] &quot;british&quot; &quot;time&quot;      &quot;disarma&quot;  
## [13,] &quot;organ&quot;   &quot;norwegian&quot; &quot;countri&quot;  
## [14,] &quot;speech&quot;  &quot;nation&quot;    &quot;carter&quot;   
## [15,] &quot;freedom&quot; &quot;laureat&quot;   &quot;norwegian&quot;</code></pre>
<p>We can, of course, work directly with these data structures but per
our approach in this workshop, we’re going to tidy our results and take
the data interpretation and visualization back to the tidyverse where we
have all its tools at our disposal.</p>
</div>
<div id="making-sense-of-and-visualizing-output" class="section level1"
number="2">
<h1><span class="header-section-number">2</span> Making sense of and
visualizing output</h1>
<p>Let’s first plot the top words in each topic. This is generally where
you want to start in evaluating a topic model – are the topics
interpretable. We use <code>tidy()</code> to transform the beta matrix
into tidy format (one word per row) and then it is a simple task for us
to plot it in ggplot.</p>
<pre class="r"><code>terms &lt;- tidy(nobel_tm, matrix = &quot;beta&quot;)
words_in_topics &lt;- terms %&gt;%
  group_by(topic) %&gt;%
  slice_max(beta, n = 10) %&gt;% 
  ungroup() %&gt;%
  arrange(topic, -beta)
words_in_topics %&gt;%
  mutate(term = reorder_within(term, beta, topic)) %&gt;%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = &quot;free&quot;) +
  scale_y_reordered()</code></pre>
<p><img src="3_2-Topicmodelling_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Let’s turn to the matrix of probabilities of topics over documents.
To keep us on our toes <code>topicmodels</code> calls this not theta but
<code>gamma</code> (<span class="math inline">\(\gamma\)</span>).</p>
<pre class="r"><code>topics_in_documents &lt;- tidy(nobel_tm, matrix = &quot;gamma&quot;)
topics_in_documents</code></pre>
<pre><code>## # A tibble: 1,410 × 3
##    document topic      gamma
##    &lt;chr&gt;    &lt;int&gt;      &lt;dbl&gt;
##  1 1981         1 0.00000702
##  2 2017         1 0.00000999
##  3 1954         1 0.0000101 
##  4 1925         1 0.00000764
##  5 1926         1 0.00000764
##  6 1968         1 1.00      
##  7 2013         1 0.0000143 
##  8 1988         1 0.0000121 
##  9 1953         1 0.00000627
## 10 2016         1 0.0000123 
## # … with 1,400 more rows
## # ℹ Use `print(n = ...)` to see more rows</code></pre>
<p>This tells us the estimated proportion of words in each given
document devoted (generated by) to a specific topic. A problem here is
that numbering topics makes it hard to figure out what this means. So we
can first rename the topics. We can do this by hand (recommended) or
automatically based on the highest ranking words in the previous beta
matrix.</p>
<pre class="r"><code># labelling by hand, we would extend this to 1:20, and given 20 topics if we wanted to name them all
#hand_topics &lt;- tibble(old_topic = 1:3, new_topic = c(&quot;International peace&quot;, &quot;Nuclear&quot;, &quot;Peac and war&quot;))
#topics_in_documents %&gt;%
#  left_join(hand_topics_topics, by=c(&quot;topic&quot; = &quot;old_topic&quot;))

# alternative two, easier for demonstration purposes on a sub-optimally-fit topic model
(auto_topics &lt;- apply(terms(nobel_tm, 3), 2, paste, collapse = &quot;-&quot;))  # pastes together the top three terms for each topic in the nobel topic model</code></pre>
<pre><code>##                Topic 1                Topic 2                Topic 3 
##   &quot;peac-countri-right&quot;  &quot;peac-nuclear-weapon&quot;      &quot;peac-nation-war&quot; 
##                Topic 4                Topic 5                Topic 6 
##      &quot;peac-war-refuge&quot;     &quot;peac-countri-war&quot;     &quot;peac-world-prize&quot; 
##                Topic 7                Topic 8                Topic 9 
##      &quot;war-nation-peac&quot;   &quot;refuge-war-countri&quot;     &quot;peac-nobel-prize&quot; 
##               Topic 10               Topic 11               Topic 12 
##      &quot;war-peac-nation&quot;     &quot;peac-human-prize&quot; &quot;nation-peac-american&quot; 
##               Topic 13               Topic 14               Topic 15 
##     &quot;leagu-nation-war&quot;     &quot;peac-prize-nobel&quot;  &quot;nuclear-peac-weapon&quot;</code></pre>
<pre class="r"><code>(auto_topics &lt;- tibble(old_topic = 1:k, new_topic = auto_topics)) # make as tibble where numeric topics are matched with the auto generated ones</code></pre>
<pre><code>## # A tibble: 15 × 2
##    old_topic new_topic           
##        &lt;int&gt; &lt;chr&gt;               
##  1         1 peac-countri-right  
##  2         2 peac-nuclear-weapon 
##  3         3 peac-nation-war     
##  4         4 peac-war-refuge     
##  5         5 peac-countri-war    
##  6         6 peac-world-prize    
##  7         7 war-nation-peac     
##  8         8 refuge-war-countri  
##  9         9 peac-nobel-prize    
## 10        10 war-peac-nation     
## 11        11 peac-human-prize    
## 12        12 nation-peac-american
## 13        13 leagu-nation-war    
## 14        14 peac-prize-nobel    
## 15        15 nuclear-peac-weapon</code></pre>
<pre class="r"><code>(topics &lt;- topics_in_documents %&gt;%
  left_join(auto_topics, by=c(&quot;topic&quot; = &quot;old_topic&quot;)))</code></pre>
<pre><code>## # A tibble: 1,410 × 4
##    document topic      gamma new_topic         
##    &lt;chr&gt;    &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;             
##  1 1981         1 0.00000702 peac-countri-right
##  2 2017         1 0.00000999 peac-countri-right
##  3 1954         1 0.0000101  peac-countri-right
##  4 1925         1 0.00000764 peac-countri-right
##  5 1926         1 0.00000764 peac-countri-right
##  6 1968         1 1.00       peac-countri-right
##  7 2013         1 0.0000143  peac-countri-right
##  8 1988         1 0.0000121  peac-countri-right
##  9 1953         1 0.00000627 peac-countri-right
## 10 2016         1 0.0000123  peac-countri-right
## # … with 1,400 more rows
## # ℹ Use `print(n = ...)` to see more rows</code></pre>
<p>Now we have our data in a familiar format we can subset and
visualize. Perhaps we’d like to compare the topic distribution in
several topics.</p>
<pre class="r"><code>topics %&gt;%
  filter(document %in% c(1977, 1985, 1996)) %&gt;%  # the documents we want to compare
  ggplot(aes(new_topic, gamma, fill = document)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~ document, ncol = 3)</code></pre>
<p><img src="3_2-Topicmodelling_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>We can visualize the distribution of all topics over time.</p>
<pre class="r"><code>topics %&gt;%
  ggplot(aes(document, gamma)) +
    geom_col(aes(group = new_topic, fill = new_topic)) +
    scale_x_discrete(breaks = seq(1905, 2019, 10))</code></pre>
<p><img src="3_2-Topicmodelling_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>Or look at the distribution of specific topics over time.</p>
<pre class="r"><code># This one requires a more balanced topic mixture to be very meaningful, which the Nobel corpus with its current fit does to have
topics %&gt;%
  filter(str_detect(new_topic, &quot;war&quot;)) %&gt;%
  ggplot(aes(document, gamma)) +
  geom_line(aes(group = new_topic, color = new_topic)) +
  scale_x_discrete(breaks = seq(1905, 2019, 10))</code></pre>
<p><img src="3_2-Topicmodelling_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
</div>
<div id="stm" class="section level1" number="3">
<h1><span class="header-section-number">3</span> STM</h1>
<p>There are several packages in R that fit topic models, most notably
<code>stm</code> which is incorporates a host of handy visualization
tools as well as the capacity to incorporate covariates into the model
fit.</p>
<p><img src="data/stm.png" width="70%" /> <span
class="citation">Roberts, Stewart, and Tingley (2019)</span>.</p>
<p>Here’s a brief example of how we can use <code>quanteda</code> to
create a document-feature matrix and then throw that into
<code>stm</code>.</p>
<pre class="r"><code>nobel &lt;- read_rds(&quot;nobel_cleaned.Rds&quot;)
nobel_decade &lt;- nobel %&gt;%
  mutate(decade = Year %/% 10 * 10) %&gt;%
  mutate(Period = ifelse(Year &lt;= 1945, &quot;Pre-WWII&quot;, &quot;Post-WWII&quot;)) %&gt;%
  corpus(text_field = &#39;AwardSpeech&#39;) %&gt;%
  tokens(remove_numbers = TRUE, remove_punc = TRUE) %&gt;%
  dfm() %&gt;%
  dfm_remove(pattern = stop_words$word) %&gt;%
  dfm_group(groups = decade)
fit10 &lt;- stm(nobel_decade, K = 10, max.em.its = 5, init.type = &quot;Spectral&quot;)
plot(fit10)</code></pre>
<p><img src="3_2-Topicmodelling_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>This shows us the ten topics we requested it find and the expected
proportions. The topics dominate individual documents it looks like,
making this perhaps not the best example, but one we’ll do just to show
the mechanics of <code>stm</code>.</p>
<pre class="r"><code>nobel_periods &lt;- nobel %&gt;%
  #mutate(decade = Year %/% 10 * 10) %&gt;%
  mutate(Period = ifelse(Year &lt;= 1945, &quot;Pre-WWII&quot;, &quot;Post-WWII&quot;)) %&gt;%
  corpus(text_field = &#39;AwardSpeech&#39;) %&gt;%
  tokens(remove_numbers = TRUE, remove_punc = TRUE) %&gt;%
  dfm() %&gt;%
  dfm_remove(pattern = stop_words$word)
# setting EM iterations to 10 for speed -- normally this should be many more (often set at 75)
fit10_period &lt;- stm(nobel_periods, K = 10, content =~Period, prevalence =~ Period, max.em.its = 5, init.type = &quot;Spectral&quot;)
plot(fit10_period)</code></pre>
<p><img src="3_2-Topicmodelling_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>We have now fitted this topic model with the covariable
<code>Period</code> denoting pre- or post-WWII and this then allows us
to further examine, for example, for each topic the words in the topic
that are more associated with one variable or the other.</p>
<pre class="r"><code>plot(fit10_period, type = &quot;perspectives&quot;, topics = 9)</code></pre>
<p><img src="3_2-Topicmodelling_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<div id="excercises" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> Excercises</h2>
<ul>
<li>Run a topic model on the sustainability report corpus. How can we
deal with year variables when they are not the name of the
document?</li>
<li>Experiment more with the Nobel corpus. Can you find a better/more
meaningful model fit?</li>
</ul>
</div>
<div id="other-topic-modeling-resources" class="section level2"
number="3.2">
<h2><span class="header-section-number">3.2</span> Other topic modeling
resources</h2>
<p>This is only the most basic of introductions to topic modeling. For
more information on topic modeling and analysis in the tidyverse, see
chapter 6 of <span class="citation">Silge and Robinson
(2017)</span>.</p>
<p>For a good explainer on topic models, see <span
class="citation">Underwood (2012)</span>.</p>
</div>
</div>
<div id="cosine-frequency" class="section level1" number="4">
<h1><span class="header-section-number">4</span> Cosine frequency</h1>
<p>Another way to think about differences between documents is cosine
frequency. If we imagine each document being represented by a vector in
n-dimensional space, where the values of the vector are the counts (more
often tf-idfs in practice) of words contained in the document, then we
can use very simple math to measure the angle of distance between two
document vectors. This is refered to as cosine difference - stemming
from the law of cosines and closely related to the <em>dot product</em>
for those who remember that from math class. Fittingly for cosines, 1
denotes that the vectors are pointing the exact same direction and that
there is no different – what would happen if measured the distace
between two documents or multiples of the same document. Zero denotes no
similarity – we would say they are <em>orthogonal</em> – there is no
overlap in vocabulary. Important to remember in interpreting cosine
distances is that they are only significant between the two documents
you are comparing – if document A has low cosine distance to both
documents B and C, this does not denote that B and C are similar, they
are simply both not similar to A (they might be dissimilar in quite
different ways.)</p>
<p>Let’s look at a quick implementation.</p>
<pre class="r"><code>(cosine_diff &lt;- textstat_simil(nobel_decade, method = &quot;cosine&quot;))</code></pre>
<pre><code>## textstat_simil object; method = &quot;cosine&quot;
##       1900  1910  1920  1930  1940  1950  1960  1970  1980  1990  2000  2010
## 1900 1.000 0.676 0.453 0.598 0.561 0.442 0.441 0.609 0.518 0.551 0.571 0.553
## 1910 0.676 1.000 0.387 0.549 0.514 0.406 0.387 0.545 0.458 0.469 0.488 0.458
## 1920 0.453 0.387 1.000 0.762 0.710 0.686 0.571 0.602 0.579 0.508 0.503 0.519
## 1930 0.598 0.549 0.762 1.000 0.747 0.690 0.605 0.675 0.651 0.588 0.612 0.585
## 1940 0.561 0.514 0.710 0.747 1.000 0.711 0.663 0.693 0.645 0.582 0.623 0.579
## 1950 0.442 0.406 0.686 0.690 0.711 1.000 0.644 0.611 0.658 0.530 0.533 0.512
## 1960 0.441 0.387 0.571 0.605 0.663 0.644 1.000 0.681 0.694 0.584 0.614 0.615
## 1970 0.609 0.545 0.602 0.675 0.693 0.611 0.681 1.000 0.796 0.777 0.790 0.746
## 1980 0.518 0.458 0.579 0.651 0.645 0.658 0.694 0.796 1.000 0.759 0.785 0.736
## 1990 0.551 0.469 0.508 0.588 0.582 0.530 0.584 0.777 0.759 1.000 0.806 0.766
## 2000 0.571 0.488 0.503 0.612 0.623 0.533 0.614 0.790 0.785 0.806 1.000 0.803
## 2010 0.553 0.458 0.519 0.585 0.579 0.512 0.615 0.746 0.736 0.766 0.803 1.000</code></pre>
<p>We have used <code>quanteda</code> to do this and with it come a
bunch of nice features (you see above how easy it was to stopword).
<code>Quanteda</code> also makes it easy to trim our corpus in ways that
might make more sense to run analysis on.</p>
<pre class="r"><code>dfm_small &lt;- dfm_trim(nobel_decade, min_termfreq = 3, min_docfreq = 2)</code></pre>
<p><code>Quanteda</code> has multiples ways to test proximity, distance,
and so on. One way of visualizing distance might be clustering – to take
all documents (here the documents are each decade’s worth of Nobel award
speeches) and break them into two clusters based on their Euclidean
distance, to do it again, and again, and so forth.</p>
<pre class="r"><code>eu_dist &lt;- textstat_dist(dfm_weight(nobel_decade, scheme = &quot;prop&quot;))
cluster &lt;- hclust(as.dist(eu_dist))
cluster$labels &lt;- docnames(nobel_decade)
plot(cluster, xlab = &quot;&quot;, sub = &quot;&quot;, main = &quot;Clustered Euclidean Distance&quot;)</code></pre>
<p><img src="3_2-Topicmodelling_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>Finally, another way to envision differences between documents is to
calculate the “keyness”, which analyses relative term frequency between
documents. Here we make the target group post-WWII Nobel speeches, and
the baseline those from 1905 up to WWII.</p>
<pre class="r"><code>keyness &lt;- textstat_keyness(nobel_decade, target = nobel_decade$decade &gt;= 1945)
textplot_keyness(keyness)</code></pre>
<p><img src="3_2-Topicmodelling_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-roberts2019stm" class="csl-entry">
Roberts, Margaret E, Brandon M Stewart, and Dustin Tingley. 2019.
<span>“Stm: An r Package for Structural Topic Models.”</span>
<em>Journal of Statistical Software</em> 91 (1): 1–40. <a
href="https://rdrr.io/cran/stm/f/inst/doc/stmVignette.pdf">https://rdrr.io/cran/stm/f/inst/doc/stmVignette.pdf</a>.
</div>
<div id="ref-silge2017text" class="csl-entry">
Silge, Julia, and David Robinson. 2017. <em>Text Mining with
<span>R</span>: A Tidy Approach</em>. "O’Reilly Media, Inc.". <a
href="https://www.tidytextmining.com/">https://www.tidytextmining.com/</a>.
</div>
<div id="ref-underwood2012" class="csl-entry">
Underwood, Ted. 2012. <span>“Topic Modeling Made Just Simple
Enough.”</span> 2012. <a
href="https://tedunderwood.com/2012/04/07/topic-modeling-made-just-simple-enough/">https://tedunderwood.com/2012/04/07/topic-modeling-made-just-simple-enough/</a>.
</div>
</div>
</div>
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p>Perhaps the most popular topic modeling package in R has
now become <code>stm</code> – see <a
href="https://juliasilge.com/blog/sherlock-holmes-stm/">these</a> <a
href="https://juliasilge.com/blog/evaluating-stm/">nice</a> blog posts
by Julia Silge for examples of working with <code>stm</code> through
<code>tidytext</code>.<a href="#fnref1"
class="footnote-back">↩︎</a></p></li>
</ol>
</div>

<br><br><p>2022.


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
