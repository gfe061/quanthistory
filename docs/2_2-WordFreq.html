<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Session 2.2: Word frequencies and dictionary methods</title>

<script src="site_libs/header-attrs-2.15/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/spacelab.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/clipboard-1.7.1/clipboard.min.js"></script>
<link href="site_libs/primer-tooltips-1.4.0/build.css" rel="stylesheet" />
<link href="site_libs/klippy-0.0.0.9500/css/klippy.min.css" rel="stylesheet" />
<script src="site_libs/klippy-0.0.0.9500/js/klippy.min.js"></script>
<script src="site_libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<link href="site_libs/wordcloud2-0.0.1/wordcloud.css" rel="stylesheet" />
<script src="site_libs/wordcloud2-0.0.1/wordcloud2-all.js"></script>
<script src="site_libs/wordcloud2-0.0.1/hover.js"></script>
<script src="site_libs/wordcloud2-binding-0.2.1/wordcloud2.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>





<style type="text/css">
/* for pandoc --citeproc since 2.11 */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="styles.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-inverse  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html"></a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">
    <span class="fa fa-home"></span>
     
    Overview
  </a>
</li>
<li>
  <a href="pensum.html">
    <span class="fa fa-list-ul"></span>
     
    Reading List
  </a>
</li>
<li>
  <a href="schedule.html">
    <span class="fa fa-info"></span>
     
    Schedule
  </a>
</li>
<li>
  <a href="1_RBasics.html">
    <span class="fa fa-book"></span>
     
    Session 1
  </a>
</li>
<li>
  <a href="2_ScrapingAndReadingInDocuments.html">
    <span class="fa fa-book"></span>
     
    Session 2
  </a>
</li>
<li>
  <a href="3_CleaningAndManipulating.html">
    <span class="fa fa-book"></span>
     
    Session 3
  </a>
</li>
<li>
  <a href="4_WordFreq.html">
    <span class="fa fa-book"></span>
     
    Session 4
  </a>
</li>
<li>
  <a href="5_SentAndDict.html">
    <span class="fa fa-book"></span>
     
    Session 5
  </a>
</li>
<li>
  <a href="6_DocSimil.html">
    <span class="fa fa-book"></span>
     
    Session 6
  </a>
</li>
<li>
  <a href="7_topicmodelling.html">
    <span class="fa fa-book"></span>
     
    Session 7
  </a>
</li>
<li>
  <a href="8_WordEmbedding.html">
    <span class="fa fa-book"></span>
     
    Session 8
  </a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Session 2.2: Word frequencies and
dictionary methods</h1>

</div>


<script>
  addClassKlippyTo("pre.r, pre.markdown");
  addKlippy('right', 'top', 'auto', '1', 'Copy code', 'Copied!');
</script>
<div id="tidy-corpus-form" class="section level1" number="1">
<h1><span class="header-section-number">1</span> Tidy corpus form</h1>
<p>We’ve talked about tidy format – each variable a column, each
observation a row, each type of observational unit a table. We’ve taken
speeches at the Nobel prize and pages of oil company Sustainability
Reports as our observations and can use this to do some transformations
– do total word counts or specific word frequencies. But once we start
really doing analysis at the level of the word, tidy rules would seem to
dictate that we treat not individual texts but individual <em>words</em>
as observations. Thus tidytext format is a table with one word per
row.</p>
<p>It might help if we are more specific: we can use two signifiers for
words - words and tokens. Words are unique, well, words as we conceive
of them. Tokens are each individual instances of a word. So a sentence:
“the brown fox jumped over the brown log” we will say has 8 tokens
(number of total words) but 6 words (number of unique words, as “the”
and “brown” appear twice).<a href="#fn1" class="footnote-ref"
id="fnref1"><sup>1</sup></a> So tidy data format is <em>one token per
row</em>.<a href="#fn2" class="footnote-ref"
id="fnref2"><sup>2</sup></a></p>
<p>We convert character strings that we have been working with into tidy
text with the <code>unnest_rows()</code> function in the
<code>tidytext package</code>, which splits texts up by tokens (a
process appropriately called “tokenization”).</p>
<pre class="r"><code># Lets use the first speech in our Nobel corpus as an example
library(tidytext)
library(tidyverse)
nobel &lt;- read_rds(&quot;data/nobel_cleaned.Rds&quot;)
ex &lt;- nobel[1,]
ex %&gt;%
  unnest_tokens(output = words, input = AwardSpeech)</code></pre>
<pre><code>## # A tibble: 598 × 3
##     Year Laureate           words       
##    &lt;dbl&gt; &lt;chr&gt;              &lt;chr&gt;       
##  1  1905 Bertha von Suttner on          
##  2  1905 Bertha von Suttner behalf      
##  3  1905 Bertha von Suttner of          
##  4  1905 Bertha von Suttner the         
##  5  1905 Bertha von Suttner nobel       
##  6  1905 Bertha von Suttner committee   
##  7  1905 Bertha von Suttner bjørnstjerne
##  8  1905 Bertha von Suttner bjørnson    
##  9  1905 Bertha von Suttner introduced  
## 10  1905 Bertha von Suttner the         
## # … with 588 more rows
## # ℹ Use `print(n = ...)` to see more rows</code></pre>
<p>We now end up with a dataframe where each row is an observation with
words, year, and Nobel laureate as variables. Note also that
<code>tidytext</code> does cleaning - whitespace stripping, to_lower and
so on. We didn’t notice here as we’re using our dataframe that we have
already cleaned. The first thing we might notice is the number of rows
is the total word count.</p>
</div>
<div id="stop-words" class="section level1" number="2">
<h1><span class="header-section-number">2</span> Stop Words</h1>
<p>Another way in which texts are frequently pre-processed for analysis
and we can now look at is to remove so-called stop words. Stop words are
words with grammatical rather than syntactic function - they make things
we say grammatical but don’t add (much) meaning. Examples include “a”,
“the”, “of” and so on. What counts as a stopword might vary on our
corpus! And what sort of analysis we’re trying to do.<a href="#fn3"
class="footnote-ref" id="fnref3"><sup>3</sup></a> All major text
analysis packages have means for removing stop words. In ‘’tidytext’’ we
have a list of stopwords called <code>stop_words</code> from a couple of
different corpora (<code>?stop_words</code> for more info and
links).</p>
<pre class="r"><code>stop_words</code></pre>
<pre><code>## # A tibble: 1,149 × 2
##    word        lexicon
##    &lt;chr&gt;       &lt;chr&gt;  
##  1 a           SMART  
##  2 a&#39;s         SMART  
##  3 able        SMART  
##  4 about       SMART  
##  5 above       SMART  
##  6 according   SMART  
##  7 accordingly SMART  
##  8 across      SMART  
##  9 actually    SMART  
## 10 after       SMART  
## # … with 1,139 more rows
## # ℹ Use `print(n = ...)` to see more rows</code></pre>
<pre class="r"><code>nobel_tidy &lt;- nobel %&gt;%
  unnest_tokens(output = words, input = AwardSpeech) %&gt;%
  anti_join(stop_words, by = c(&quot;words&quot; = &quot;word&quot;))  # by= specifies which columns to use, had they been named the same thing we could have omitted it</code></pre>
<p><code>stop_words</code>, we’ll recall, is a tibble, so we can easily
make our own tibble of stopwords and add it. Say (for purposes of
demonstration) I think that the Peace Prize award ceremony is, of
course, going to talk about peace and war so we want to weed these words
out of our frequency counts.</p>
<pre class="r"><code>my_words &lt;- c(&quot;peace&quot;, &quot;war&quot;)
custom_stop_words &lt;- tibble(word = my_words, lexicon = &quot;my_customization&quot;)
stop_words_custom &lt;- rbind(stop_words, custom_stop_words)
tail(stop_words_custom) # view the end of the tibble, look like our words were added correctly</code></pre>
<pre><code>## # A tibble: 6 × 2
##   word     lexicon         
##   &lt;chr&gt;    &lt;chr&gt;           
## 1 younger  onix            
## 2 youngest onix            
## 3 your     onix            
## 4 yours    onix            
## 5 peace    my_customization
## 6 war      my_customization</code></pre>
<p>Now we can apply the <code>stop_words_custom</code> just like we did
<code>stop_words</code>. We won’t actually do this because we probably
want these words in the corpus!</p>
</div>
<div id="stemming" class="section level1" number="3">
<h1><span class="header-section-number">3</span> Stemming</h1>
<p>If we are looking at word frequencies, will we want to count
“represent” and “represented” as the same word? Or “war” and “wars”?
Very likely. If so we can transform our text via a process called
stemming, cutting down words to their stems so that different forms of
these word are recognized as being the same thing. Something like this
matters in English but might really matter in an more highly inflected
language like Russian.</p>
<p>We have a couple stemmers to choose from in R, one and the best known
is the Porter Stemming <a
href="https://tartarus.org/martin/PorterStemmer/">algorithm</a>. Another
is <a
href="https://docs.ropensci.org/hunspell/articles/intro.html">hunspell</a>,
based on the popular open source and multilingual spell checker. We’ll
use Porter here.</p>
<p>To see it in action we’ll first test it against a short test set of
words and then apply to our whole corpus.</p>
<pre class="r"><code>library(SnowballC)
words_to_stem &lt;- c(&quot;going&quot;, &quot;represented&quot;, &quot;wars&quot;, &quot;similarity&quot;, &quot;books&quot;)
SnowballC::wordStem(words_to_stem)</code></pre>
<pre><code>## [1] &quot;go&quot;      &quot;repres&quot;  &quot;war&quot;     &quot;similar&quot; &quot;book&quot;</code></pre>
<p>Stemming in action. So now applied to the entire document:</p>
<pre class="r"><code>(nobel_tidy_stemmed &lt;- nobel_tidy %&gt;%
  mutate(word_stem = wordStem(words)))</code></pre>
<pre><code>## # A tibble: 94,107 × 4
##     Year Laureate           words        word_stem  
##    &lt;dbl&gt; &lt;chr&gt;              &lt;chr&gt;        &lt;chr&gt;      
##  1  1905 Bertha von Suttner behalf       behalf     
##  2  1905 Bertha von Suttner nobel        nobel      
##  3  1905 Bertha von Suttner committee    committe   
##  4  1905 Bertha von Suttner bjørnstjerne bjørnstjern
##  5  1905 Bertha von Suttner bjørnson     bjørnson   
##  6  1905 Bertha von Suttner introduced   introduc   
##  7  1905 Bertha von Suttner speaker      speaker    
##  8  1905 Bertha von Suttner baroness     baro       
##  9  1905 Bertha von Suttner bertha       bertha     
## 10  1905 Bertha von Suttner von          von        
## # … with 94,097 more rows
## # ℹ Use `print(n = ...)` to see more rows</code></pre>
<pre class="r"><code>write_rds(nobel_tidy_stemmed, &quot;data/nobel_stemmed.Rds&quot;)</code></pre>
</div>
<div id="top-word-frequencies" class="section level1" number="4">
<h1><span class="header-section-number">4</span> Top word frequencies<a
href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a></h1>
<p>Now we can find the most frequently appearing words in the
corpus.</p>
<pre class="r"><code>nobel_tidy %&gt;%
  count(words, sort=TRUE)</code></pre>
<pre><code>## # A tibble: 12,329 × 2
##    words             n
##    &lt;chr&gt;         &lt;int&gt;
##  1 peace          1595
##  2 war             927
##  3 world           847
##  4 prize           646
##  5 nations         642
##  6 nobel           597
##  7 international   530
##  8 people          513
##  9 time            441
## 10 human           440
## # … with 12,319 more rows
## # ℹ Use `print(n = ...)` to see more rows</code></pre>
<p>Remember we got this nice and informative result only because we
already removed the stopwords. OTherwise we would have been swamped with
“the”, “and”, and so on.</p>
<p>Our knowledge of how to subset tibbles will now come in pretty handy.
If we want to get the most frequent words, say, before 1945 we easily do
this.</p>
<pre class="r"><code>nobel_tidy %&gt;%
  filter(Year &lt; 1945) %&gt;%
  count(words, sort=TRUE)</code></pre>
<pre><code>## # A tibble: 4,801 × 2
##    words             n
##    &lt;chr&gt;         &lt;int&gt;
##  1 peace           346
##  2 war             301
##  3 nations         193
##  4 international   157
##  5 league          145
##  6 world           110
##  7 time             81
##  8 american         72
##  9 prize            72
## 10 europe           69
## # … with 4,791 more rows
## # ℹ Use `print(n = ...)` to see more rows</code></pre>
<p>And we could then compare it to post-1945 word frequency and we’d see
pre-WWII prize speeches were more concentrated on Europe, the League,
and nations while after that we get more talk of world, people, human,
committee. That makes sense.</p>
<p>Let’s graph this.</p>
<pre class="r"><code>nobel_tidy %&gt;%
  count(words, sort=TRUE) %&gt;%
  top_n(15) %&gt;%                     # selecting to show only top 15 words
  mutate(words = reorder(words,desc(n))) %&gt;%  # this will ensure that the highest frequency words appear to the left
  ggplot(aes(words, n)) +
    geom_col()</code></pre>
<pre><code>## Selecting by n</code></pre>
<p><img src="2_2-WordFreq_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>And with just a little bit more code we can view pre-1945 and
post-1945 top frequency words at the same time.</p>
<pre class="r"><code>nobel_tidy %&gt;%
  mutate(Period = ifelse(Year &lt;= 1945, &quot;Pre-WWII&quot;, &quot;Post-WWII&quot;)) %&gt;%   # creating columns with label &quot;Pre-WWII&quot; and &quot;Post-WWII&quot;
  mutate(Period = factor(Period, levels = c(&quot;Pre-WWII&quot;, &quot;Post-WWII&quot;))) %&gt;%
  group_by(Period) %&gt;%                                                # grouping by this column label so frequencies will be                                                                              calculated within group
  count(words, sort=TRUE) %&gt;%
  mutate(proportion = n / sum(n) * 1000) %&gt;%                     # perhaps we&#39;d like word frequency per 1000 words rather than raw                                                                      counts?
  slice_max(order_by=proportion, n = 15) %&gt;%                     # selecting to show only top 15 words within each group
  ggplot(aes(reorder_within(x = words, by = proportion, within = Period), proportion, fill = Period)) +    # reordering is a bit tricky, see                                                                                                     ?reorder_within()
    geom_col() +
    scale_x_reordered() +
    coord_flip() +
    facet_wrap(~Period, ncol = 2, scales = &quot;free&quot;) +
  xlab(&quot;Word&quot;)</code></pre>
<p><img src="2_2-WordFreq_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<div id="word-clouds" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Word Clouds</h2>
<p>One common visualization of word frequency is word clouds. To do this
we use the package wordcloud which will work very nicely with our tidily
organized data. Wordcloud2 gives color and more fancy options that you
can also play with.</p>
<pre class="r"><code>library(wordcloud)</code></pre>
<pre><code>## Loading required package: RColorBrewer</code></pre>
<pre class="r"><code>library(wordcloud2)</code></pre>
<pre><code>## Warning: package &#39;wordcloud2&#39; was built under R version 4.2.2</code></pre>
<pre class="r"><code>nobel_tidy %&gt;%
  count(words, sort=TRUE) %&gt;%
  with(wordcloud(words, n, max.words = 100))</code></pre>
<p><img src="2_2-WordFreq_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<pre class="r"><code>dat &lt;- nobel_tidy %&gt;%
  count(words, sort=TRUE) %&gt;%
  mutate(word = words) %&gt;%
  mutate(freq = n) %&gt;%
  select(word, freq) %&gt;%
  top_n(200)</code></pre>
<pre><code>## Selecting by freq</code></pre>
<pre class="r"><code>wordcloud2(dat, size = 2)</code></pre>
<div id="htmlwidget-9913b713b3d8e65134c4" style="width:672px;height:480px;" class="wordcloud2 html-widget"></div>
<script type="application/json" data-for="htmlwidget-9913b713b3d8e65134c4">{"x":{"word":["peace","war","world","prize","nations","nobel","international","people","time","human","countries","committee","united","rights","political","life","nuclear","country","norwegian","league","weapons","president","europe","government","women","refugees","future","organization","social","hope","economic","national","efforts","policy","american","struggle","disarmament","awarded","award","words","conflict","means","day","history","agreement","conference","freedom","south","africa","power","year's","security","support","peaceful","secretary","children","development","union","cooperation","germany","live","violence","called","office","opinion","spirit","situation","difficult","task","east","european","military","movement","respect","society","arms","conditions","force","law","democracy","france","minister","powers","america","foreign","form","position","public","declaration","free","common","forces","result","found","wars","fight","idea","parties","set","french","justice","hand","laureate","population","process","contribution","governments","question","assembly","established","mankind","solution","council","organizations","politics","true","community","million","active","soviet","speech","treaty","negotiations","individual","1","aid","alfred","british","german","achieved","fear","based","relations","sense","carried","importance","responsibility","system","gentlemen","view","1945","action","conflicts","brought","civil","goal","held","ladies","plan","prisoners","reconciliation","ideas","living","received","basis","west","2","born","personal","race","times","achieve","age","created","fundamental","lead","4","geneva","laureates","nobel's","party","reason","strong","bring","father","peoples","progress","convention","nation","period","major","practical","reality","cross","faith","led","principles","step","african","attitude","campaign","courage","global","home","labor","mind","prevent","prime","road","special"],"freq":[1595,927,847,646,642,597,530,513,441,440,435,399,382,357,320,307,278,277,261,246,215,208,206,197,195,194,184,177,176,170,168,166,165,165,164,161,158,151,149,142,139,136,135,135,134,134,133,133,131,130,128,127,122,121,121,120,120,120,118,118,117,117,115,114,114,113,112,111,111,110,106,106,106,105,104,103,102,102,101,99,98,98,98,97,96,96,95,94,93,93,92,92,92,91,91,90,90,90,90,89,89,88,88,88,88,86,86,86,85,85,85,85,84,84,84,84,83,83,82,82,82,82,80,79,78,78,78,78,78,77,77,76,76,76,75,75,75,75,73,73,72,72,72,71,71,71,71,71,71,71,70,69,69,69,68,68,67,67,67,67,67,66,66,66,66,66,65,65,65,65,65,65,65,64,64,64,64,63,63,63,62,62,62,61,61,61,61,61,60,60,60,60,60,60,60,60,60,60,60,60],"fontFamily":"Segoe UI","fontWeight":"bold","color":"random-dark","minSize":0,"weightFactor":0.225705329153605,"backgroundColor":"white","gridSize":0,"minRotation":-0.785398163397448,"maxRotation":0.785398163397448,"shuffle":true,"rotateRatio":0.4,"shape":"circle","ellipticity":0.65,"figBase64":null,"hover":null},"evals":[],"jsHooks":{"render":[{"code":"function(el,x){\n                        console.log(123);\n                        if(!iii){\n                          window.location.reload();\n                          iii = False;\n\n                        }\n  }","data":null}]}}</script>
<p>It’s also possible to do word clouds that compare two documents. To
do this we’ll need to step outside the tidyverse and organize our data
in other formats so we save this for Session 5.</p>
</div>
</div>
<div id="tf-idf" class="section level1" number="5">
<h1><span class="header-section-number">5</span> TF-IDF</h1>
<p>As we’ve seen, there are multiple ways to calculate frequency – we
can take raw counts, or term frequency (<span
class="math inline">\(tf\)</span>). Proportions, term frequency divided
by total token count in a given document, are another means. Certainly
proportions make it easier to compare across corpora of different size.
The problem with this is that they tend to get flooded with stopwords.
One means of dealing with this is to remove the stopwords as we have
done, but another is to attempt to downweigh words that appear often
everywhere and upweigh those that are more unusual. Inverse document
frequency (<span class="math inline">\(idf\)</span>) is a weighting
system to do this – it equals the total number of documents in the
corpus divided by the number of documents in the corpus that contain the
given word. The greater the number of documents in the corpus in which
the word does <em>not</em> appear (suggesting words that are unique to
certain documents rather than widespread across the corpus as a whole)
the smaller the denominator and, thus, the greater the ratio.</p>
<p>TF-IDF is is term frequency times inverse document frequency. Both
are often logged. In symbols,</p>
<p><span class="math display">\[\begin{equation}
   \text{TF}_{t,d} =
\begin{cases}
   1 + \text{log}_{10} \: \text{count(t,d)}, &amp; \text{if count(t,d)}
&gt; 0 \\
   0,&amp;\text{otherwise.}\\
\end{cases}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
   \text{IDF}_{t} = \text{log}_{10} \:
\bigg(\frac{N}{\text{df}_t}\bigg).
\end{equation}\]</span></p>
<p>where t is the given term, d is a given document, df<span
class="math inline">\(_t\)</span> is the number of documents in the
corpus containing term t, and N is the total number of documents in the
corpus. Long story short, tf-idf attempts to weight words by both
frequency in an individual document and their unusualness over a corpus
of documents. Every word in every document will have its own tf-idf
(term frequency will vary across documents while inverse document
frequency is the same across the corpus).</p>
<p>Let’s see how we do this in R. First we’ll compute document
frequency. In order to simplify results, lets use the same subsetting of
our data into pre-1945 and post-1945 – this means we’re treating
pre-WWII speeches as one single document and likewise post-1945
speeches. <code>tidytext</code> makes it pretty easy – simply unnest
tokens and then count the tokens. Note that <code>count</code> enable
counting within groups, which we passed to <code>count</code> telling it
to do the counts within the groups denoted in column
<code>Period</code>. This produces the same result as passing
<code>group_by(Period)</code> in the previous line and eliminating
<code>Period</code> from the <code>count()</code> call.</p>
<pre class="r"><code>nobel &lt;- read_rds(&quot;data/nobel_cleaned.Rds&quot;) %&gt;%
  mutate(Period = ifelse(Year &lt;= 1945, &quot;Pre-WWII&quot;, &quot;Post-WWII&quot;))
nobel_words &lt;- nobel %&gt;%
  unnest_tokens(words, AwardSpeech) %&gt;%
  count(words, Period, sort = TRUE)</code></pre>
<p>We can then use <code>bind_tf_idf()</code> from tidytext. (Tidytext
implements tf-idf using proportional, but not logged, tf – we’ll see
some of these other versions in other packages). The function takes a
first argument (other than the tidy dataframe) that is the word, a
second that is the document, and third a column containing document-term
counts).</p>
<pre class="r"><code>tf_idf &lt;- nobel_words %&gt;%
  bind_tf_idf(words, Period, n)
tf_idf</code></pre>
<pre><code>## # A tibble: 17,036 × 6
##    words Period        n     tf   idf tf_idf
##    &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
##  1 the   Post-WWII 13947 0.0742     0      0
##  2 of    Post-WWII  7281 0.0387     0      0
##  3 in    Post-WWII  5614 0.0299     0      0
##  4 to    Post-WWII  5591 0.0297     0      0
##  5 and   Post-WWII  5417 0.0288     0      0
##  6 a     Post-WWII  3712 0.0197     0      0
##  7 the   Pre-WWII   3526 0.0761     0      0
##  8 that  Post-WWII  2543 0.0135     0      0
##  9 is    Post-WWII  2329 0.0124     0      0
## 10 of    Pre-WWII   2191 0.0473     0      0
## # … with 17,026 more rows
## # ℹ Use `print(n = ...)` to see more rows</code></pre>
<p>Here we see that tf-idf has zeroed out these extremely common and not
very interesting terms, precisely what we’d hope an indicator like this
would do. Lets see the highest tf-idf scores.</p>
<pre class="r"><code>tf_idf %&gt;% arrange(desc(tf_idf)) </code></pre>
<pre><code>## # A tibble: 17,036 × 6
##    words    Period        n       tf   idf   tf_idf
##    &lt;chr&gt;    &lt;chr&gt;     &lt;int&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;
##  1 nuclear  Post-WWII   278 0.00148  0.693 0.00102 
##  2 saavedra Pre-WWII     22 0.000475 0.693 0.000329
##  3 locarno  Pre-WWII     20 0.000432 0.693 0.000299
##  4 angell   Pre-WWII     19 0.000410 0.693 0.000284
##  5 pan      Pre-WWII     19 0.000410 0.693 0.000284
##  6 ladies   Post-WWII    71 0.000378 0.693 0.000262
##  7 global   Post-WWII    60 0.000319 0.693 0.000221
##  8 non      Post-WWII    54 0.000287 0.693 0.000199
##  9 poverty  Post-WWII    52 0.000277 0.693 0.000192
## 10 persons  Post-WWII    51 0.000271 0.693 0.000188
## # … with 17,026 more rows
## # ℹ Use `print(n = ...)` to see more rows</code></pre>
<pre class="r"><code>tf_idf %&gt;%
  mutate(Period = factor(Period, levels = c(&quot;Pre-WWII&quot;, &quot;Post-WWII&quot;))) %&gt;%
  group_by(Period) %&gt;%
  slice_max(tf_idf, n = 15) %&gt;%
  ungroup() %&gt;%
  ggplot(aes(tf_idf, fct_reorder(words, tf_idf), fill = Period)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~Period, ncol = 2, scales = &quot;free&quot;) +
  labs(x = &quot;tf-idf&quot;, y = NULL)</code></pre>
<p><img src="2_2-WordFreq_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>Many of these are names, logically enough as specific names of
laureates appear mostly only when they are being awarded a prize thus
increasing their idf and upweighing their tf-idf. To make this more
useful we’d want to go through and week out (remove, just like
stopwords) names. But still we see nuclear coming to the fore in the
post-war era, reasonably enough! Things like “global”, “poverty” stand
out post-war while pre-war we see “reparations”, “commercial”, more
Europe specific vocabulary. A different conversation pre- and
post-war.</p>
</div>
<div id="pos" class="section level1" number="6">
<h1><span class="header-section-number">6</span> POS</h1>
<p>We have now calculated lists of words appearing with top frequency
and also seen how to calculate and graph words or phrases of interest
across documents. But what if we were interested in seeing highest
frequency words within a larger set of words? If were were interested in
a limited set of, say, geographical names or people we could just make
the list, do a join to remove all other words and calculate top
frequencies among this subset of words. But what if we were interested
in, say, highest occurring verbs?</p>
<p>For this and many other reasons linguists have long been interested
in writing algorithms to automatically identify parts of speech, called
part-of-speech tagging or POS. A POS tagger would take a sentence such
as “The brown fox jumped over the brown log” and return “The_(article)
brown_(adjective) fox_(noun) over_(preposition)” and so on.</p>
<p>One way we could imagine going about this is with a dictionary that
identified nouns, verbs, etc. but this is clearly naive – many a word
can be both noun and verb and much will depend on context. So we’ll use
the openNLP package and write this function<a href="#fn5"
class="footnote-ref" id="fnref5"><sup>5</sup></a></p>
<pre class="r"><code>library(NLP)
library(tm)  # load before openNLP
library(openNLP)
library(openNLPmodels.en)

POStagger &lt;- function(text, filter = NULL){
  # defining annotators 
  sent_token_annotator &lt;- Maxent_Sent_Token_Annotator()
  word_token_annotator &lt;- Maxent_Word_Token_Annotator()
  pos_tag_annotator &lt;- Maxent_POS_Tag_Annotator()
  # make sure input is a string
  text &lt;- as.String(text)
  # defining a pipeline
  annotate_text &lt;- NLP::annotate(text, Annotator_Pipeline(
    sent_token_annotator,
    word_token_annotator,
    pos_tag_annotator
  ))
  # generate tags
  tags &lt;- sapply(subset(annotate_text, type==&quot;word&quot;)$features, `[[`, &quot;POS&quot;)
  # get tokens
  tokens &lt;- text[subset(annotate_text, type==&quot;word&quot;)]
  # filter out just one pos type
  if (!is.null(filter)) {
    tagged &lt;- tokens[tags %in% filter]
  } else {
  # put tokens and pos tags in df
  tagged &lt;- tibble(word=tokens, pos=tags) %&gt;% 
      filter(!str_detect(pos, pattern=&#39;[[:punct:]]&#39;))}
  return(tagged)
}

nobel &lt;- read_rds(&quot;data/nobel_cleaned.Rds&quot;)
POStagger(nobel$text[1])</code></pre>
<p>This yields a nice dataframe with tokens in the first column and POS
in the second column. The POS are not just “noun”, “verb” and so on but
more finely differentiated. See <a
href="https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html">here</a>
for a list of abbreviations. Using the same filter, group_by, etc
methods we can now find lists of most frequently words by type of
speech. There are, of course, other applications we might imagine when
POS-tagging is used in conjunction with other topics discussed in the
workshop.</p>
<p>You don’t really need to worry about what’s in there but that it
takes a character string (preferably cleaned, as always, to limit the
chance of unexpected errors). Note, too, that the function has the
option of specifying a filter that will return a string of words of a
certain POS.</p>
</div>
<div id="sentiment-analysis" class="section level1" number="7">
<h1><span class="header-section-number">7</span> Sentiment analysis</h1>
<p>Another commonly done sort of analysis that we can easily incorporate
into our tidy workflow is that of sentiment analysis. There are numerous
ways computational linguists have developed to algorithmically determine
the “sentiment” of a piece of text. We focus here on a simple (and
fairly naive) method that works on the level of individual words and
employs a sentiment dictionary, essentially a list of words and their
associated sentiments. As we will see, these same dictionary methods can
also be applied to any list or lists of words and thus give analysts a
more flexible tool to track vocabulary across a corpus.</p>
<p><code>tidytext</code> includes three dictionaries, each working
slightly differently.<a href="#fn6" class="footnote-ref"
id="fnref6"><sup>6</sup></a> Let’s take a look below.</p>
<pre class="r"><code>library(tidytext)
# the first time you will need to say yes to download of the sentiment dictionary
get_sentiments(&quot;afinn&quot;)</code></pre>
<pre><code>## # A tibble: 2,477 × 2
##    word       value
##    &lt;chr&gt;      &lt;dbl&gt;
##  1 abandon       -2
##  2 abandoned     -2
##  3 abandons      -2
##  4 abducted      -2
##  5 abduction     -2
##  6 abductions    -2
##  7 abhor         -3
##  8 abhorred      -3
##  9 abhorrent     -3
## 10 abhors        -3
## # … with 2,467 more rows
## # ℹ Use `print(n = ...)` to see more rows</code></pre>
<pre class="r"><code>tail(get_sentiments(&quot;afinn&quot;))</code></pre>
<pre><code>## # A tibble: 6 × 2
##   word     value
##   &lt;chr&gt;    &lt;dbl&gt;
## 1 youthful     2
## 2 yucky       -2
## 3 yummy        3
## 4 zealot      -2
## 5 zealots     -2
## 6 zealous      2</code></pre>
<p>Here we have negative or positive sentiment (words beginning with
“ab-” seem to be quite negative!). If we want to get a quick overview of
the scale we can call either <code>table()</code> or
<code>summary()</code></p>
<pre class="r"><code>table(get_sentiments(&quot;afinn&quot;)$value) # for categorical data, tells us categories and n of those categories</code></pre>
<pre><code>## 
##  -5  -4  -3  -2  -1   0   1   2   3   4   5 
##  16  43 264 966 309   1 208 448 172  45   5</code></pre>
<pre class="r"><code>summary(get_sentiments(&quot;afinn&quot;)$value) # summary statistics for the value column </code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## -5.0000 -2.0000 -2.0000 -0.5894  2.0000  5.0000</code></pre>
<p>So we see the value of sentiments goes from -5 to 5. Calling the
other two:</p>
<pre class="r"><code>get_sentiments(&quot;bing&quot;)</code></pre>
<pre><code>## # A tibble: 6,786 × 2
##    word        sentiment
##    &lt;chr&gt;       &lt;chr&gt;    
##  1 2-faces     negative 
##  2 abnormal    negative 
##  3 abolish     negative 
##  4 abominable  negative 
##  5 abominably  negative 
##  6 abominate   negative 
##  7 abomination negative 
##  8 abort       negative 
##  9 aborted     negative 
## 10 aborts      negative 
## # … with 6,776 more rows
## # ℹ Use `print(n = ...)` to see more rows</code></pre>
<pre class="r"><code>get_sentiments(&quot;nrc&quot;)</code></pre>
<pre><code>## # A tibble: 13,872 × 2
##    word        sentiment
##    &lt;chr&gt;       &lt;chr&gt;    
##  1 abacus      trust    
##  2 abandon     fear     
##  3 abandon     negative 
##  4 abandon     sadness  
##  5 abandoned   anger    
##  6 abandoned   fear     
##  7 abandoned   negative 
##  8 abandoned   sadness  
##  9 abandonment anger    
## 10 abandonment fear     
## # … with 13,862 more rows
## # ℹ Use `print(n = ...)` to see more rows</code></pre>
<p>“Bing” labels words simply negative or positive and the “nrc” lexicon
labels them according to a handful of main emotions. You can take a look
at the tidytext documentation for more background on the lexicons.<a
href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a></p>
<p>So what do we do with these? The simplest method is simply to count
words from the lexicons that appear in a text and add them all up. This
is what is known as a dictionary method, which we’ll talk about in a
little more depth below. From a practical standpoint, how do we add up
all dictionary words in a text? Tidy makes this pretty easy. You’ll see
the code is a little bit different based on the kind of dictionary.</p>
<p>For the <code>bing</code> dictionary we will tokenize our corpus, do
an inner join<a href="#fn8" class="footnote-ref"
id="fnref8"><sup>8</sup></a> with the sentiments dictionary which will
create a new row in our Nobel corpus dataframe where every row is the
associated sentiment with the corpus word and corpus words that do not
have a sentiment associated with them in the <code>bing</code>
dictionary will be eliminated from the dataframe. We then do a
<code>pivot_wider</code>, the opposite of <code>pivot_longer</code>
which we did in the previous session when graphing n-grams.
<code>pivot_wider</code> will make several columns out of fewer – here
taking the values of the <code>sentiment</code> column (“positive” and
“negative”) and making them column titles and populating the column
values with those from the associated rows of our <code>n</code> column.
Take a look at what the dataframe looks like before and after
transformation to make sure you understand what is happening. We do this
pivot in order to then be able to do basic arithmetic – take the number
of “positive” word frequencies and subtract the number of negative word
frequencies, and do this for every document in the corpus. This will be
our sentiment “score”. It is then a fairly straightforward path to
graphing it on ggplot.</p>
<pre class="r"><code>library(tidyverse)
nobel &lt;- read_rds(&quot;data/nobel_cleaned.Rds&quot;)
# calculating text sentiment by subtracting total positive sentiment words from total negative with bing lexicon
nobel %&gt;%
  unnest_tokens(word, AwardSpeech) %&gt;%  ## we call our new column &quot;word&quot; which makes inner_joins easier 
  inner_join(get_sentiments(&quot;bing&quot;)) %&gt;%
  count(sentiment, year = Year) %&gt;% 
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;% 
  mutate(sentiment = positive - negative) %&gt;%
  ggplot(aes(year, sentiment)) +
           geom_line(show.legend = FALSE) +
           geom_hline(yintercept = 0, linetype = 2, alpha = .8)</code></pre>
<p><img src="2_2-WordFreq_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>With the <code>afinn</code> corpus we have numeric “sentiment values”
attached to words so we need to sum all the values of all words in each
document.</p>
<pre class="r"><code># using the afinn lexicon
nobel %&gt;%
  unnest_tokens(word, AwardSpeech) %&gt;%  ## we call our new column &quot;word&quot; which makes inner_joins easier 
  inner_join(get_sentiments(&quot;afinn&quot;)) %&gt;%
  group_by(Year) %&gt;%
  summarize(sentiment = sum(value)) %&gt;%
    ggplot(aes(Year, sentiment)) +
    geom_line(show.legend = FALSE) +
    geom_hline(yintercept = 0, linetype = 2, alpha = .8)</code></pre>
<p><img src="2_2-WordFreq_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>We can also do this over different categories over years. For
instance, say we’d like to do this same kind of chart but for each of
our oil company’s sustainability reports.</p>
<pre class="r"><code>sr &lt;- srps &lt;- read_csv(&quot;data/srps.csv&quot;)</code></pre>
<pre><code>## Rows: 3543 Columns: 3
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &quot;,&quot;
## chr (2): Text, Company
## dbl (1): Year
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.</code></pre>
<pre class="r"><code>sr %&gt;%
  unnest_tokens(word, Text) %&gt;%  ## we call our new column &quot;word&quot; which makes inner_joins easier 
  inner_join(get_sentiments(&quot;afinn&quot;)) %&gt;%
  group_by(Company, Year) %&gt;%
  summarize(sentiment = sum(value)) %&gt;%
  ggplot(aes(Year, sentiment)) +
    geom_line(show.legend = FALSE) +
    geom_hline(yintercept = 0, linetype = 2, alpha = .8) +
    facet_wrap(~Company, ncol = 2, scales = &quot;free_x&quot;)</code></pre>
<pre><code>## Joining, by = &quot;word&quot;
## `summarise()` has grouped output by &#39;Company&#39;. You can override using the
## `.groups` argument.</code></pre>
<p><img src="2_2-WordFreq_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>We can also use the sentiment lexicons in ways that mostly combine
what we have already done in subsetting dataframes and counting words.
Say we’d like to see which words in a certain category of documents are
associated with negative anticipation in the pre-WWII vs post-WWII Nobel
prize award speeches. We’ll do this in several steps.</p>
<p>First we get a dictionary of the words associated with anticipation.
Then do the same with negative sentiment words. Then we again do an
<code>inner_join()</code> that creates a new dataframe that includes
only those words present in both <code>anticipation</code> and
<code>negative</code>.</p>
<pre class="r"><code>anticipation &lt;- get_sentiments(&quot;nrc&quot;) %&gt;% 
  filter(sentiment == &quot;anticipation&quot;)
negative &lt;- get_sentiments(&quot;nrc&quot;) %&gt;%
  filter(sentiment == &quot;negative&quot;)
negAnt &lt;- inner_join(anticipation, negative, by=&quot;word&quot;) %&gt;%
  select(word)        # we don&#39;t really need the other two columns which tell us that they are negative and anticipation words</code></pre>
<p>Now we make our Nobel data “tidy” (one-token-per-row) and do another
<code>inner_join()</code> with our negative anticipation words. What we
have left are all negative anticipation words in the corpus, which we
can then print.</p>
<pre class="r"><code>nobel %&gt;%
  mutate(Period = ifelse(Year &gt;= 1945, &quot;Pre-WWII&quot;, &quot;Post-WWII&quot;)) %&gt;%
  mutate(Period = factor(Period, levels = c(&quot;Pre-WWII&quot;, &quot;Post-WWII&quot;))) %&gt;% # CHanges
  unnest_tokens(word, AwardSpeech) %&gt;%
  inner_join(negAnt) %&gt;%
  group_by(Period) %&gt;%
  count(word, sort = TRUE) %&gt;%
  slice_max(order_by=n, n = 15) %&gt;%                     # selecting to show only top 15 words within each group
  ggplot(aes(reorder_within(x = word, by = n, within = Period), n, fill = Period)) +    # reordering is a bit tricky, see ?reorder_within()
  geom_col(show.legend = FALSE) +
  scale_x_reordered() +
  coord_flip() +
  facet_wrap(~Period, ncol = 2, scales = &quot;free&quot;) +
  theme_bw() +
  xlab(&#39;Words&#39;)</code></pre>
<pre><code>## Joining, by = &quot;word&quot;</code></pre>
<p><img src="2_2-WordFreq_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>Wait, “mother” is a word associated with both negativity and
anticipation?</p>
<pre class="r"><code>get_sentiments(&quot;nrc&quot;) %&gt;%
  filter(word == &quot;mother&quot;)</code></pre>
<pre><code>## # A tibble: 6 × 2
##   word   sentiment   
##   &lt;chr&gt;  &lt;chr&gt;       
## 1 mother anticipation
## 2 mother joy         
## 3 mother negative    
## 4 mother positive    
## 5 mother sadness     
## 6 mother trust</code></pre>
<p>Apparently so, according to this lexicon. Which offers us a nice
segue into a larger discussion of dictionaries.</p>
</div>
<div id="dictionary-methods" class="section level1" number="8">
<h1><span class="header-section-number">8</span> Dictionary methods</h1>
<p>It is entirely obvious to historians that the meaning of words – not
to mention latent sentiments – changes over time, place, and context
within individual documents. So these methods have to be used with care.
To take one instance, economists have noticed that words like
“liability” have very different sentiment and latent connotations in
finance and other areas of life (<span class="citation">Loughran and
Mcdonald (2011)</span>). Thus it might behoove us to create our own
dictionaries and this is something we could easily do. In this case, we
(as domain experts) could compile our own dictionaries not only to judge
sentiment, but to measure attention given to certain topics, tone of
discussion,</p>
<p>Here really the only challenge is to read a dictionary into R and
then use it per the above methods. Say that we have a list of words in a
text file, not comma-separated, that is a dictionary for identifying
when the theme of oil is brought up in a text (this is something I came
up with in literally 5 seconds, please do not use this for anything –
actual dictionaries should be considerably better thought out, as well
as verified on real texts that it’s catching what you want it to). Each
word is on its own separate line (coincidentally this is the format of
the above cited Norwegian sentiment dictionary, so should you want to
use that you can adopt the process here.)</p>
<pre class="r"><code>oil_dict &lt;- read_lines(&quot;data/oil_theme.txt&quot;)
oil_dict &lt;- tibble(word = oil_dict, dictionary = &quot;oil dictionary&quot;)
oil_dict %&gt;%
  filter(word != &quot;&quot;)</code></pre>
<pre><code>## # A tibble: 6 × 2
##   word      dictionary    
##   &lt;chr&gt;     &lt;chr&gt;         
## 1 oil       oil dictionary
## 2 petrol    oil dictionary
## 3 petroleum oil dictionary
## 4 gas       oil dictionary
## 5 tanker    oil dictionary
## 6 fossil    oil dictionary</code></pre>
<p>In all of two lines of code we’ve made it into a tibble. We can see
also we have an empty string as the last line. We take care of that and
then we’re all set to use this dictionary. Now if we want to add up
everytime a text uses a word in this dictionary we can easily do
this.</p>
<div id="excercises" class="section level2" number="8.1">
<h2><span class="header-section-number">8.1</span> Excercises</h2>
<ul>
<li>Compile your own dictionary of interest and use it to locate a theme
of interest in the Nobel corpus.</li>
<li>Use our very informal oil dictionary and apply it to the
Sustainability Reports. You might also scale by number of total words
per corpus – are oil terms smaller proportion of sustainability reports
than they were 10 years ago?</li>
</ul>
</div>
</div>
<div id="mulitple-word-analysis" class="section level1" number="9">
<h1><span class="header-section-number">9</span> Mulitple word
analysis</h1>
<div id="n-grams" class="section level2" number="9.1">
<h2><span class="header-section-number">9.1</span> N-grams</h2>
<p>In Google’s famous n-grams viewer, you can search not just for a
single word but for phrases, sets of words “n” tokens long. Indeed, this
is why it is called an “n”-gram, they are consecutive sequences of an
arbitrary number of words. So far we’ve only been looking at individual
words, so lets think about multi-word units.</p>
<p>True to form, the tidyverse will help out with this, allowing us to
<code>unnest_tokens()</code> by the n-gram by telling R that our token
of interest is now not the single word but an ngram of n length.</p>
<pre class="r"><code>nobel %&gt;%
  unnest_tokens(twogram, AwardSpeech, token = &quot;ngrams&quot;, n = 2)</code></pre>
<pre><code>## # A tibble: 234,267 × 3
##     Year Laureate           twogram               
##    &lt;dbl&gt; &lt;chr&gt;              &lt;chr&gt;                 
##  1  1905 Bertha von Suttner on behalf             
##  2  1905 Bertha von Suttner behalf of             
##  3  1905 Bertha von Suttner of the                
##  4  1905 Bertha von Suttner the nobel             
##  5  1905 Bertha von Suttner nobel committee       
##  6  1905 Bertha von Suttner committee bjørnstjerne
##  7  1905 Bertha von Suttner bjørnstjerne bjørnson 
##  8  1905 Bertha von Suttner bjørnson introduced   
##  9  1905 Bertha von Suttner introduced the        
## 10  1905 Bertha von Suttner the speaker           
## # … with 234,257 more rows
## # ℹ Use `print(n = ...)` to see more rows</code></pre>
<p>And we could then make a plot of the most frequently occurring
2-grams. The problem with this is that we’re going to be overrun with
stopwords, but now that we have bigrams how to do take the individual
stop words? We could search and remove with <code>str_remove_all</code>
without unnesting words, we could unnest, take out the stop words,
renest and then unnest by 2-grams, but we could also use a more tidy
approach and <code>separate()</code> which is another handy command for
manipulating tidyverse dataframes.</p>
<pre class="r"><code>nobel %&gt;%
  unnest_tokens(twogram, AwardSpeech, token = &quot;ngrams&quot;, n = 2) %&gt;%
  separate(twogram, into=c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;)  # here the call states what col to separate, into which columns, and where the separation should be made, here when there is a space between the words</code></pre>
<pre><code>## # A tibble: 234,267 × 4
##     Year Laureate           word1        word2       
##    &lt;dbl&gt; &lt;chr&gt;              &lt;chr&gt;        &lt;chr&gt;       
##  1  1905 Bertha von Suttner on           behalf      
##  2  1905 Bertha von Suttner behalf       of          
##  3  1905 Bertha von Suttner of           the         
##  4  1905 Bertha von Suttner the          nobel       
##  5  1905 Bertha von Suttner nobel        committee   
##  6  1905 Bertha von Suttner committee    bjørnstjerne
##  7  1905 Bertha von Suttner bjørnstjerne bjørnson    
##  8  1905 Bertha von Suttner bjørnson     introduced  
##  9  1905 Bertha von Suttner introduced   the         
## 10  1905 Bertha von Suttner the          speaker     
## # … with 234,257 more rows
## # ℹ Use `print(n = ...)` to see more rows</code></pre>
<p>Now we can do our stopword work. We could do an
<code>anti_join</code> on word1 and then word2 but the other nifty thing
we can do with tidy commands is use the <code>%in%</code> command (this
being the syntax of commands used in SQL databases some of which the
tidyverse lets you do).</p>
<pre class="r"><code>nob &lt;- nobel %&gt;%
  mutate(Period = ifelse(Year &lt;= 1945, &quot;Pre-WWII&quot;, &quot;Post-WWII&quot;)) %&gt;% 
  unnest_tokens(twogram, AwardSpeech, token = &quot;ngrams&quot;, n = 2) %&gt;%
  separate(twogram, into=c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) %&gt;%
  filter(!word1 %in% stop_words$word) %&gt;%
  filter(!word2 %in% stop_words$word) %&gt;%
  unite(twogram, word1, word2, sep = &#39; &#39;) %&gt;%              # now putting word1 and word2 back into a single column called twogram
  group_by(Period) %&gt;%
  count(twogram, sort=TRUE) %&gt;%
  slice_max(order_by=n, n = 10)  

nob %&gt;%
  mutate(twogram = reorder(twogram, n)) %&gt;%
  ungroup() %&gt;%
  ggplot(aes(reorder_within(x = twogram, by = n, within = Period), n, fill = Period)) +
    geom_col(show.legend = FALSE) +
  scale_x_reordered() +  
  coord_flip() +
    facet_wrap(~Period, ncol = 2, scales = &quot;free&quot;) +
    ylab(&#39;n&#39;) +
    xlab(&quot;2-gram&quot;)</code></pre>
<p><img src="2_2-WordFreq_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
</div>
<div id="co-occurance" class="section level2" number="9.2">
<h2><span class="header-section-number">9.2</span> Co-occurance</h2>
<p>Words can also co-occur in the same context even if they are not
necessarily right next to each other. To get at this we’ll need more
than n-grams. One way to do this is the widyr package, which essentially
restructures our tidy data does an operation and the recasts it back
into a tidy format.<a href="#fn9" class="footnote-ref"
id="fnref9"><sup>9</sup></a></p>
<p>For instance, we can use the <code>pairwise_count()</code> function
to find all the words that appear together in the same Nobel speech (not
necessarily right next to each other).</p>
<pre class="r"><code>library(widyr)
nobel %&gt;%
  unnest_tokens(word, AwardSpeech) %&gt;%
  filter(!word %in% stop_words$word) %&gt;%
  pairwise_count(word, Year, sort = TRUE)</code></pre>
<pre><code>## # A tibble: 22,374,876 × 3
##    item1 item2     n
##    &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;
##  1 time  peace    89
##  2 peace time     89
##  3 prize peace    86
##  4 peace prize    86
##  5 peace nobel    85
##  6 nobel peace    85
##  7 war   peace    85
##  8 world peace    85
##  9 peace war      85
## 10 peace world    85
## # … with 22,374,866 more rows
## # ℹ Use `print(n = ...)` to see more rows</code></pre>
<p>Notice that this is a much larger dataframe – our unnested dataframe
minus stopwords was ~93.000 words, this new one is +21 million.</p>
<p>From here we might do many things using the same skills of
subsetting, counting, and plotting we have talked to up to now. We
filter for words co-occurring with “evil” and plot the most frequently
occurring in the same document as “evil”. We might also note that
<code>unnest_tokens()</code> will can also tokenize by sentence. This
means that if you wanted to look at co-occurrence within sentences you
could tokenize by sentence and give each sentence (which would occupy
one row each) an index number (something like
<code>mutate(index = row_number())</code> might do the trick) and then
call <code>pairwise_count</code>.</p>
<div id="excercises-1" class="section level3" number="9.2.1">
<h3><span class="header-section-number">9.2.1</span> Excercises</h3>
<ul>
<li>Implement the above suggestion – tokenize by sentence and search for
most frequently co-occurring words with keywords of your choice. Try
this for either Nobel or SR corpora (NB. The SR dataframe is also
structured one page of text per row, thus you could also look for
co-occurrence within individual pages of the Sustainability
Reports.)</li>
<li>Find most frequently occurring n-grams (of whatever size n) in the
SR corpus. Any surprises?</li>
</ul>
</div>
</div>
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-clark2018text" class="csl-entry">
Clark, Michael. 2018. <span>“An Introduction to Text Processing and
Analysis with <span>R</span>.”</span> 2018. <a
href="https://m-clark.github.io/text-analysis-with-R/">https://m-clark.github.io/text-analysis-with-R/</a>.
</div>
<div id="ref-jurafsky2014speech" class="csl-entry">
Jurafsky, Dan, and James H Martin. forthcoming. <span>“Speech and
Language Processing. Vol. 3.”</span> Pearson London London. <a
href="http://www.web.stanford.edu/~jurafsky/slp3/">http://www.web.stanford.edu/~jurafsky/slp3/</a>.
</div>
<div id="ref-loughran_when_2011" class="csl-entry">
Loughran, Tim, and Bill Mcdonald. 2011. <span>“When <span>Is</span> a
<span>Liability</span> <span>Not</span> a <span>Liability</span>?
<span>Textual</span> <span>Analysis</span>, <span>Dictionaries</span>,
and 10-<span>Ks</span>.”</span> <em>The Journal of Finance</em> 66 (1):
35–65.
</div>
<div id="ref-mosteller_inference_1963" class="csl-entry">
Mosteller, Frederick, and David L. Wallace. 1963. <span>“Inference in an
<span>Authorship</span> <span>Problem</span>.”</span> <em>Journal of the
American Statistical Association</em> 58 (302): 275. <a
href="https://doi.org/10.2307/2283270">https://doi.org/10.2307/2283270</a>.
</div>
<div id="ref-niekler2020" class="csl-entry">
Niekler, Andreas, and Gregor Wiedemann. 2020. <span>“Text Mining in
<span>R</span> for the Social Sciences and Digital Humanities.”</span>
2020. <a
href="https://tm4ss.github.io/docs/index.html">https://tm4ss.github.io/docs/index.html</a>.
</div>
<div id="ref-schweinberger2021pos" class="csl-entry">
Schweinberger, Martin. 2021. <span>“POS-Tagging and Syntactic Parsing
with <span>R</span>.”</span> 2021. <a
href="https://slcladal.github.io/tagging.html#2_POS-Tagging_with_openNLP">https://slcladal.github.io/tagging.html#2_POS-Tagging_with_openNLP</a>.
</div>
<div id="ref-silge2017text" class="csl-entry">
Silge, Julia, and David Robinson. 2017. <em>Text Mining with
<span>R</span>: A Tidy Approach</em>. "O’Reilly Media, Inc.". <a
href="https://www.tidytextmining.com/">https://www.tidytextmining.com/</a>.
</div>
<div id="ref-wickham2016r" class="csl-entry">
Wickham, Hadley, and Garrett Grolemund. 2016. <em><span>R</span> for
Data Science: Import, Tidy, Transform, Visualize, and Model Data</em>. "
O’Reilly Media, Inc.". <a
href="https://r4ds.had.co.nz/index.html">https://r4ds.had.co.nz/index.html</a>.
</div>
</div>
</div>
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p>We could also start talking about tokens and words being
not just things we’d recognize as words but things like :) and so on.<a
href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>We will also later talk about n-grams where we might
take tokens to be 2-grams, sentences or even longer pieces of text.<a
href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>Interestingly, one of the most famous cases of
computational text analysis in the social science – Mosteller and
Wallace’s attribution of anonymously published Federalist papers –
analyzed stopwords and threw out everything else (<span
class="citation">Mosteller and Wallace (1963)</span>).<a href="#fnref3"
class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>This section and the next lean heavily on chapters 1 and
3 in <span class="citation">Silge and Robinson (2017)</span><a
href="#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>Based on examples from <span class="citation">Clark
(2018)</span>, <span class="citation">Schweinberger (2021)</span>, and
especially <span class="citation">Niekler and Wiedemann (2020)</span>.<a
href="#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>these are English-language, it should be said. There are
many for other languages out there, see <a
href="https://github.com/ltgoslo/norsentlex">here</a> for an extensive
Norwegian positive/negative sentiment dictionary. Some of these will
involve a little wrangling of the data to read into R and get them into
tidy format.<a href="#fnref6" class="footnote-back">↩︎</a></p></li>
<li id="fn7"><p>For some discussion of the nrc lexicon and more
background on sentiment analysis, see chapter 20 in <span
class="citation">Jurafsky and Martin (forthcoming)</span>. One of the
earliest and best known sentiment dictionaries is the <a
href="http://www.wjh.harvard.edu/~inquirer/spreadsheet_guide.htm">General
Inquirer</a>.<a href="#fnref7" class="footnote-back">↩︎</a></p></li>
<li id="fn8"><p>See chapter 13 in <span class="citation">Wickham and
Grolemund (2016)</span> for great visualizations of joins and
translation into R commands.<a href="#fnref8"
class="footnote-back">↩︎</a></p></li>
<li id="fn9"><p>There are simple statistics to test the correlation
between words in the same document (phi-test – in widyr as
<code>pairwise_corr</code>), statistics to measure differences in word
use between documents (log-likelihood, chi-squared, keyness), and so on.
These are a bit much to cover in a two day workshop but area all fairly
easy to implement in R and you can easily find tutorials and explainers
that will help you do this.<a href="#fnref9"
class="footnote-back">↩︎</a></p></li>
</ol>
</div>

<br><br><p>2022.


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
